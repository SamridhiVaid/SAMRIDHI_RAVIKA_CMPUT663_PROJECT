"""# Checking on Python dataset"""
# -*- coding: utf-8 -*-
"""SA_Project_stacking classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DnqIcbdhG32qSTUNlaCKsw38R-IBVyNK
"""



# Commented out IPython magic to ensure Python compatibility.
# %cd SAMRIDHI_RAVIKA_CMPUT663_PROJECT

"""# Ensemble Learning on Cado Dataset"""

import scripts.dataset_utils as du

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import numpy as np
from sklearn.metrics import classification_report, hamming_loss, accuracy_score
from sklearn.metrics import average_precision_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.ensemble import StackingClassifier
from sklearn.multiclass import OneVsRestClassifier
from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain
from sklearn.ensemble import RandomForestClassifier
import time
import os
import sys

MAX_NB_WORDS =20000

def tokenize_data(X):
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(X)
    return tokenizer

def preprocess_data():

    print("Preprocessing data...")
    data_path = 'datasets/python/python_stdlib_clean.csv'
    data = du.load_data_encoded(data_path)
    text_index = 1
    label_start_index = 2
    X = [d[text_index] for d in data]
    labels = [d[label_start_index:label_start_index + 12] for d in data]
    Y = np.array(labels, dtype='int')
    test_index = 70
    tokenizer = tokenize_data(X)
    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(X)
    X = pad_sequences(sequences, maxlen=700,
                  padding="post", truncating="post", value=0)
    num_words = min(MAX_NB_WORDS, len(word_index) + 1)
    embedding_matrix = np.zeros((num_words, 1))
    for word, i in word_index.items():
        if i >= MAX_NB_WORDS:
            continue
        embedding_matrix[i] = 1
    X_train = X[0:test_index, :]
    Y_train = Y[0:test_index, :]
    x_test = X[test_index:len(X), :]
    y_test = Y[test_index:len(Y), :]
    print("Preprocessing complete.")
    return X_train, Y_train, x_test, y_test

"""**Stacking Classifier**"""

# Stacking classifier with one vs Rest wrapper and MLKnn - Model 1
def run_stacking_classifier():
    print("Running model...")
    estimators_list = [('ExtraTrees', ExtraTreesClassifier(n_estimators=30,
                                                       class_weight="balanced",
                                                       random_state=4621)),
                   ('linearSVC', SVC(class_weight="balanced", probability=True))]
    estimators_ensemble = StackingClassifier(estimators=estimators_list,
                                         final_estimator = LogisticRegression(max_iter=300))
    ovr_model = OneVsRestClassifier(estimators_ensemble)
    ovr_model.fit(X_train, Y_train)
    predictions = ovr_model.predict(x_test)
    my_metrics= classification_report(y_test, predictions)
    print(my_metrics)
    print ("Hamming Loss:" )
    print(hamming_loss(y_test,predictions))
    print("Accuracy Score: ")
    print(accuracy_score(y_test,predictions))

    average_precision = dict()
    for i in range(12):
        average_precision[i] = average_precision_score(y_test[:, i], predictions[:, i])

    print("Average Precision:")
    print(average_precision)

"""**BinaryRelevance**"""

# binary relevance + RandomClassifier

def run_binary_relevance():
    print("Running model...")
    start=time.time()
    classifier = BinaryRelevance(classifier = RandomForestClassifier(),require_dense = [False, True])
    classifier.fit(X_train, Y_train)
    print('training time taken: ',round(time.time()-start,0),'seconds')
    predictions1 = classifier.predict(x_test)
    my_metrics1= classification_report(y_test, predictions1)
    print(my_metrics1)
    print ("Hamming Loss:" )
    print(hamming_loss(y_test,predictions1))
    print("Accuracy Score: ")
    print(accuracy_score(y_test,predictions1))
    pred = predictions1.toarray()
    average_precision = dict()
    for i in range(12):
        average_precision[i] = average_precision_score(y_test[:, i], pred[:, i])
    print("Average Precision:")
    print(average_precision)



"""**Label Powerset**"""

def run_label_powerset():
    print("Running model...")
    classifier = LabelPowerset( classifier = SVC(), require_dense = [False, True])
    start=time.time()
    classifier.fit(X_train, Y_train)
    print('training time taken: ',round(time.time()-start,0),'seconds')
    predictions1 = classifier.predict(x_test)
    my_metrics1= classification_report(y_test, predictions1)
    print(my_metrics1)
    print ("Hamming Loss:" )
    print(hamming_loss(y_test,predictions1))
    print("Accuracy Score: ")
    print(accuracy_score(y_test,predictions1))
    pred = predictions1.toarray()
    average_precision = dict()
    for i in range(12):
        average_precision[i] = average_precision_score(y_test[:, i], pred[:, i])
    print("Average Precision:")
    print(average_precision)

"""

**ClassifierChain**
"""

def run_classifier_chain():
    print("Running model...")
    classifier = ClassifierChain(classifier = RandomForestClassifier(),require_dense = [False, True],
                                 order=[i for i in range(12)])
    start=time.time()
    classifier.fit(X_train,Y_train)
    predictions1 = classifier.predict(x_test)
    my_metrics1= classification_report(y_test, predictions1)
    print(my_metrics1)
    print("Hamming Loss:")
    print(hamming_loss(y_test, predictions1))
    print("Accuracy Score: ")
    print(accuracy_score(y_test, predictions1))
    pred = predictions1.toarray()
    average_precision = dict()
    for i in range(12):
        average_precision[i] = average_precision_score(y_test[:, i], pred[:, i])
    print("Average Precision:")
    print(average_precision)


## main method##


if (len(sys.argv) < 2 ):
    print("No optional argument passed. Pass an optional argument to choose a model to run. ")
    exit()
else:
    model_arg = sys.argv[1]

# calling different methods with optional arguments
X_train, Y_train, x_test, y_test = preprocess_data()
if model_arg == "--stacking":
    run_stacking_classifier()
elif model_arg == "--binaryrelevance":
    run_binary_relevance()
elif model_arg == "--labelpowerset":
    run_label_powerset()
elif model_arg == "--classifierchain":
    run_classifier_chain()
